---
title: "Homework 5"
author: "Xinyi Lin"
date: "11/24/2018"
output: github_document
---

```{r}
library(tidyverse)
library(faraway)
library(leaps)
```

## Input and tidy data

```{r}
data(state)
head(state.x77)
```

```{r}
state_clean_df =
  as.tibble(state.x77) %>% 
  janitor::clean_names()

state_clean_df 
```

## Question 1-a)

### Backward

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction='backward')
```

### Forward 

```{r}
start_fit = lm(life_exp ~ 1, data = state_clean_df)
step(start_fit, direction = 'forward', scope = list(upper = all_fit, lower = start_fit))
```

### Stepwise

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction = 'both')
```

According to the results, when using three methods, we get same 'best subset' which 'population, murder, hs_grad and frost'. 

## Question 1-b)

```{r}
fitted_model = lm(formula = life_exp ~ population + murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_model)

cor(state_clean_df)
```

According to the summary results, we can find the p-value of `population` variable is slightly bigger than 0.05 which is a close call, so we try to compare models with and without `population` variable. 

```{r}
fitted_less_model = lm(formula = life_exp ~ murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_less_model)
anova(fitted_model, fitted_less_model)
```

The adjuested r-square of the model without 'population' is slightly less than the adjuested r-square of the model with 'population' and AIC of the model with 'population' also perform better, so keeping the `population` variable is a better choice.

## Question 1-c)

```{r}
add_illiteracy_model = lm(formula = life_exp ~ murder + hs_grad + frost + illiteracy, 
    data = state_clean_df)

summary(add_illiteracy_model)

cor(state_clean_df)
```

The correlation of `hs_grad` and `illiteracy` is -0.657 and when adding `illiteracy` in model, the coefficient of `hs_grad` change slightly, thus there are low association between `hs_grad` and `illiteracy` and my subset only contain `hs_grad`.

## Question 3

```{r}
state_criterion_df =
  state_clean_df %>% 
  as.data.frame() %>% 
  select(life_exp, everything())

# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = state_criterion_df)
   (rs = summary(b))

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4,4,1,1))
par(mfrow = c(1,2))

plot(2:(length(rs$cp) + 1), rs$cp, xlab = "Num of parameters", ylab = "Cp Statistic")
abline(0,1)

plot(2:(length(rs$cp) + 1), rs$adjr2, xlab = "Num of parameters", ylab = "Adj R2")
```

According to the Cp and adjusted r-square results, number of parameters are 4 to 8 are better models, so we count AIC and BIC of these models.

```{r}
# AIC of the 3-predictor model:
fitted_4_model <- lm(life_exp ~ murder + hs_grad + frost, data = state_criterion_df)
AIC(fitted_4_model)

# BIC
AIC(fitted_4_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 4-predictor model:
fitted_5_model <- lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)
AIC(fitted_5_model)

# BIC
AIC(fitted_5_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 5-predictor model:
fitted_6_model <- lm(life_exp ~ murder + hs_grad + frost + population + income, data = state_criterion_df)
AIC(fitted_6_model)

# BIC
AIC(fitted_6_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 6-predictor model:
fitted_7_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy, data = state_criterion_df)
AIC(fitted_7_model)

# BIC
AIC(fitted_7_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 7-predictor model:
fitted_8_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy + area, data = state_criterion_df)
AIC(fitted_8_model)

# BIC
AIC(fitted_8_model, k = log(length(state_criterion_df$life_exp)))
```

models               | model 1 | model 2 | model 3 | model 4 | model 5
-------------------- | ------- | ------- | ------  | ------- | ------- 
number of parameters | p = 4   | p = 5   | p = 6   | p = 7   | p = 8
Cp                   | 3.7399  | 2.0197  | 4.0087  | 6.0020  | 8.0000
adjusted r-square    | 0.6939  | 0.7126  | 0.7061  | 0.6993  | 0.6922
AIC                  | 117.974 | 115.733 | 117.720 | 119.712 | 121.709
BIC                  | 127.534 | 127.205 | 131.104 | 135.008 | 138.917

Since model with 5 parameters has highest adjusted r-square, lowest AIC and BIC and its Cp lower than p, so 'best subset' is 'pulation, murder, hs_grad and frost'. 

## Question 4

Since the model selected from part 2 and part 3 is the same which is `lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)`, so the final model is `lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)`.

### leverage



### model assumptions

```{r}
par(mfrow=c(2,2))
plot(fitted_5_model)
```






