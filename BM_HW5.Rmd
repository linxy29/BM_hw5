---
title: "Homework 5"
author: "Xinyi Lin"
date: "11/24/2018"
output: github_document
---

```{r}
library(tidyverse)
library(faraway)
library(leaps)
library(caret)
library(patchwork)
library(rlist)
```

## Input and tidy data

```{r}
data(state)
head(state.x77)
```

```{r}
state_clean_df =
  as.tibble(state.x77) %>% 
  janitor::clean_names()

state_clean_df 
```

## Question 1

```{r}
summary(state_clean_df)
```

```{r}
population_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "population", y = population)) +
  geom_boxplot()
```

```{r}
illiteracy_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "illiteracy", y = illiteracy)) +
  geom_boxplot()
```

```{r}
life_exp_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "life_exp", y = life_exp)) +
  geom_boxplot()
```

```{r}
murder_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "murder", y = murder)) +
  geom_boxplot()
```

```{r}
hs_grad_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "hs_grad", y = hs_grad)) +
  geom_boxplot()
```

```{r}
frost_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "frost", y = frost)) +
  geom_boxplot()
```

```{r}
area_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "area", y = area)) +
  geom_boxplot()

(population_boxplot + illiteracy_boxplot + area_boxplot)/(murder_boxplot + hs_grad_boxplot + frost_boxplot + life_exp_boxplot)
```

## Question 2-a)

### Backward

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction='backward')
```

### Forward 

```{r}
start_fit = lm(life_exp ~ 1, data = state_clean_df)
step(start_fit, direction = 'forward', scope = list(upper = all_fit, lower = start_fit))
```

### Stepwise

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction = 'both')
```

According to the results, when using three methods, we get same 'best subset' which 'population, murder, hs_grad and frost'. 

## Question 2-b)

```{r}
fitted_model = lm(formula = life_exp ~ population + murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_model)

cor(state_clean_df)
```

According to the summary results, we can find the p-value of `population` variable is slightly bigger than 0.05 which is a close call, so we try to compare models with and without `population` variable. 

```{r}
fitted_less_model = lm(formula = life_exp ~ murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_less_model)
anova(fitted_model, fitted_less_model)
```

The adjuested r-square of the model without 'population' is slightly less than the adjuested r-square of the model with 'population' and AIC of the model with 'population' also perform better, so keeping the `population` variable is a better choice.

## Question 2-c)

```{r}
add_illiteracy_model = lm(formula = life_exp ~ murder + hs_grad + frost + illiteracy, 
    data = state_clean_df)

summary(add_illiteracy_model)

cor(state_clean_df)
```

The correlation of `hs_grad` and `illiteracy` is -0.657 and when adding `illiteracy` in model, the coefficient of `hs_grad` change slightly, thus there are low association between `hs_grad` and `illiteracy` and my subset only contain `hs_grad`.

## Question 3

```{r}
state_criterion_df =
  state_clean_df %>% 
  as.data.frame() %>% 
  select(life_exp, everything())

# Printing the best models of each size, using the Cp criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "Cp")

# Printing the best models of each size, using the adjusted R^2 criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = state_criterion_df)
   (rs = summary(b))

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4,4,1,1))
par(mfrow = c(1,2))

plot(2:(length(rs$cp) + 1), rs$cp, xlab = "Num of parameters", ylab = "Cp Statistic")
abline(0,1)

plot(2:(length(rs$cp) + 1), rs$adjr2, xlab = "Num of parameters", ylab = "Adj R2")
```

According to the Cp and adjusted r-square results, number of parameters are 4 to 8 are better models, so we count AIC and BIC of these models.

```{r}
# AIC of the 3-predictor model:
fitted_4_model <- lm(life_exp ~ murder + hs_grad + frost, data = state_criterion_df)
AIC(fitted_4_model)

# BIC
AIC(fitted_4_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 4-predictor model:
fitted_5_model <- lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)
AIC(fitted_5_model)

# BIC
AIC(fitted_5_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 5-predictor model:
fitted_6_model <- lm(life_exp ~ murder + hs_grad + frost + population + income, data = state_criterion_df)
AIC(fitted_6_model)

# BIC
AIC(fitted_6_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 6-predictor model:
fitted_7_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy, data = state_criterion_df)
AIC(fitted_7_model)

# BIC
AIC(fitted_7_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 7-predictor model:
fitted_8_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy + area, data = state_criterion_df)
AIC(fitted_8_model)

# BIC
AIC(fitted_8_model, k = log(length(state_criterion_df$life_exp)))
```

models               | model 1 | model 2 | model 3 | model 4 | model 5
-------------------- | ------- | ------- | ------  | ------- | ------- 
number of parameters | p = 4   | p = 5   | p = 6   | p = 7   | p = 8
Cp                   | 3.7399  | 2.0197  | 4.0087  | 6.0020  | 8.0000
adjusted r-square    | 0.6939  | 0.7126  | 0.7061  | 0.6993  | 0.6922
AIC                  | 117.974 | 115.733 | 117.720 | 119.712 | 121.709
BIC                  | 127.534 | 127.205 | 131.104 | 135.008 | 138.917

Since model with 5 parameters has highest adjusted r-square, lowest AIC and BIC and its Cp lower than p, so 'best subset' is 'pulation, murder, hs_grad and frost'. 

## Question 4

Since the model selected from part 2 and part 3 is the same which is `lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)`, so the final model is `lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)`.

```{r}
par(mfrow = c(2,2))
plot(fitted_5_model)
```

### leverage

According to the "Residuals vs Leverage" plot, there are no leverage in this data.

### model Assumptions

According to the 'Residuals vs Fitted' plot and 'Scale-Location` plot, we can find that residuals are randomly spread along the change of fitted values and red lines are almost striaight and horizontal, which means the residuals are almost constant across the range of Xs. However, read lines are slightly curve around 71, which means the reiduals around 71 might be slightly lower.

For 'Normal Q-Q plot', we can see all dots except first and last few dots spread around the line, considering there are only 50 observations, this is a small sample. This outliers are normal and overall, residuals are normally distributed.

## Question 5

### 10-fold cross-validation

```{r}
train_data = trainControl(method = "cv", number = 10)

# Fit the 4-variables model that we discussed in previous lectures
model_caret = 
  train(life_exp ~ murder + hs_grad + frost + population,
                   data = state_clean_df,
                   trControl = train_data,
                   method = 'lm',
                   na.action = na.pass)

model_caret
```

Model coefficients

```{r}
model_caret$finalModel
```

Results of each fold

```{r}
model_caret$resample
```

### residual sampling

Calculate predicted values and reisduals

```{r}
lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)

bootstrap_df = 
  state_clean_df %>% 
  mutate(predicted_y = 71.03 - 0.3001*murder + 0.04658*hs_grad - 0.005943*frost + 0.00005014*population)

residual_fun = function(x, y){
  return(x - y)
}

residual_l = mapply(residual_fun, bootstrap_df$life_exp, bootstrap_df$predicted_y)
```

Repeat residuals sampling and count MSE

```{r}
bootsrap_mse_fun = function(data, rep_num){
  rep_num = rep_num   # number of repetitions
  mse_v = vector(mode = "numeric", length = rep_num)
  
  len = length(residual_l)
  for (j in 1:rep_num){
    # resample residuals 
    residual_l = list.sample(residual_l, length(residual_l), replace = TRUE)
    
    # get new predicted ys and residuals
    for (n in 1:len) {
      bootstrap_df$predicted_y[n] = residual_l[n] + bootstrap_df$predicted_y[n]
      }
    
    # fit linear model
    final_model = lm(predicted_y ~ murder + hs_grad + frost + population, data = bootstrap_df)
  
    # get mse
    mse_v[j] = as.numeric(anova(final_model)["Residuals", "Mean Sq"])
    }
  # head(mse_v)
  return(summary(mse_v))
}

bootsrap_mse_fun(bootstrap_df, 10) # repeat 10 times

bootsrap_mse_fun(bootstrap_df, 1000) # repeat 10 times
```

### MSE comparing

Two RMSEs of boostrap method are both larger than MSE of 10-fold cross-validation. When using "residual sampling bootstrap", the MSE become larger and larger after each sampling, so MSEs of "residual sampling" bootstrap cannot refleact the predictivity of models and in this situation, 10-fold cross-validation is better.


