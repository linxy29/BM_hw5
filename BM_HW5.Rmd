---
title: "Homework 5"
author: "Xinyi Lin"
date: "11/24/2018"
output: pdf_document
---

```{r, message = FALSE}
library(tidyverse)
library(faraway)
library(leaps)
library(caret)
library(patchwork)
library(rlist)
library(ModelMetrics)
```

## Input and tidy data

```{r}
data(state)
head(state.x77)
```

```{r}
state_clean_df =
  as.tibble(state.x77) %>% 
  janitor::clean_names()

state_clean_df 
```

## Question 1

```{r}
summary(state_clean_df)
```

```{r}
population_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "population", y = population)) +
  geom_boxplot()
```

```{r}
income_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "income", y = income)) +
  geom_boxplot()
```

```{r}
illiteracy_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "illiteracy", y = illiteracy)) +
  geom_boxplot()
```

```{r}
life_exp_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "life_exp", y = life_exp)) +
  geom_boxplot()
```

```{r}
murder_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "murder", y = murder)) +
  geom_boxplot()
```

```{r}
hs_grad_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "hs_grad", y = hs_grad)) +
  geom_boxplot()
```

```{r}
frost_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "frost", y = frost)) +
  geom_boxplot()
```

```{r}
area_boxplot = 
  state_clean_df %>% 
  ggplot(aes(x = "area", y = area)) +
  geom_boxplot()

(population_boxplot + income_boxplot + illiteracy_boxplot + area_boxplot)/(murder_boxplot + hs_grad_boxplot + frost_boxplot + life_exp_boxplot)
```

## Question 2-a)

### Backward

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction = 'backward')
```

### Forward 

```{r}
start_fit = lm(life_exp ~ 1, data = state_clean_df)
step(start_fit, direction = 'forward', scope = list(upper = all_fit, lower = start_fit))
```

### Stepwise

```{r}
all_fit = lm(life_exp ~ ., data = state_clean_df)
step(all_fit, direction = 'both')
```

According to the results, when using three methods, we get same 'best subset' which is 'population, murder, hs_grad and frost'. Even though this three methos sometimes give different results, since they use the same criterion which is ACI. in this situation, they give the same result.

## Question 2-b)

```{r}
fitted_model = lm(formula = life_exp ~ population + murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_model)

cor(state_clean_df)
```

According to summary results, we can find the p-value of `population` variable is slightly bigger than 0.05 which is a close call, so we try to compare models with and without `population` variable. 

```{r}
fitted_less_model = lm(formula = life_exp ~ murder + hs_grad + frost, 
    data = state_clean_df)

summary(fitted_less_model)
anova(fitted_model, fitted_less_model)
```

Even though when using anova to test models with and without `population`, the F-statistics is slightly larger than 0.05, the adjuested r-square of the model without 'population' is slightly less than the adjuested r-square of the model with 'population' and AIC of the model with 'population' also perform better, so keeping the `population` variable is a better choice.

## Question 2-c)

```{r}
add_illiteracy_model1 = lm(formula = life_exp ~ murder + hs_grad + frost + illiteracy, 
    data = state_clean_df)

add_illiteracy_model2 = lm(formula = life_exp ~ murder + hs_grad + frost + illiteracy + hs_grad*illiteracy, 
    data = state_clean_df)

summary(add_illiteracy_model1)
summary(add_illiteracy_model2)

cor(state_clean_df)
```

The correlation of `hs_grad` and `illiteracy` is -0.657. When adding `illiteracy` in model, the coefficient of `hs_grad` change slightly and interaction of `hs_grad` and `illiteracy` is not significant, thus there are low association between `hs_grad` and `illiteracy` and my subset only contain `hs_grad`.

## Question 3

```{r}
state_criterion_df =
  state_clean_df %>% 
  as.data.frame() %>% 
  select(life_exp, everything())

# Printing the best models of each size, using the Cp criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "Cp")

# Printing the best models of each size, using the adjusted R^2 criterion:
leaps(x = state_criterion_df[,2:8], y = state_criterion_df[,1], nbest = 1, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = state_criterion_df)
   (rs = summary(b))

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4,4,1,1))
par(mfrow = c(1,2))

plot(2:(length(rs$cp) + 1), rs$cp, xlab = "Num of parameters", ylab = "Cp Statistic")
abline(0,1)

plot(2:(length(rs$cp) + 1), rs$adjr2, xlab = "Num of parameters", ylab = "Adj R2")
```

According to the Cp and adjusted r-square results, models with 4 to 8 parameters are better models, so we count AIC and BIC of these models.

```{r}
# AIC of the 3-predictor model:
fitted_4_model <- lm(life_exp ~ murder + hs_grad + frost, data = state_criterion_df)
AIC(fitted_4_model)

# BIC
AIC(fitted_4_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 4-predictor model:
fitted_5_model <- lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)
AIC(fitted_5_model)

# BIC
AIC(fitted_5_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 5-predictor model:
fitted_6_model <- lm(life_exp ~ murder + hs_grad + frost + population + income, data = state_criterion_df)
AIC(fitted_6_model)

# BIC
AIC(fitted_6_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 6-predictor model:
fitted_7_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy, data = state_criterion_df)
AIC(fitted_7_model)

# BIC
AIC(fitted_7_model, k = log(length(state_criterion_df$life_exp)))

# AIC of the 7-predictor model:
fitted_8_model <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy + area, data = state_criterion_df)
AIC(fitted_8_model)

# BIC
AIC(fitted_8_model, k = log(length(state_criterion_df$life_exp)))
```

models               | model 1 | model 2 | model 3 | model 4 | model 5
-------------------- | ------- | ------- | ------  | ------- | ------- 
number of parameters | p = 4   | p = 5   | p = 6   | p = 7   | p = 8
Cp                   | 3.7399  | 2.0197  | 4.0087  | 6.0020  | 8.0000
adjusted r-square    | 0.6939  | 0.7126  | 0.7061  | 0.6993  | 0.6922
AIC                  | 117.974 | 115.733 | 117.720 | 119.712 | 121.709
BIC                  | 127.534 | 127.205 | 131.104 | 135.008 | 138.917

When considering Cp, model with 4 parameters is the best, while model with 5 parameters performs better in adjusted r-square, AIC and BIC. However, adding `population` doesn't result in significant changes(6% changes) in adjusted r-square, AIC and BIC, so based on Cp and 'principle of parsimony', 'best subset' is 'murder, hs_grad and frost'. 

## Question 4

The model selected from part 2 is `lm(life_exp ~ murder + hs_grad + frost + population, data = state_criterion_df)` and the model selected form part 3 is `lm(life_exp ~ murder + hs_grad + frost, data = state_criterion_df)`. Since adding `population` doesn't result in significant changes in adjusted r-square, AIC and BIC, based on Cp and 'principle of parsimony', the final model is `lm(life_exp ~ murder + hs_grad + frost, data = state_criterion_df)`.

```{r}
par(mfrow = c(2,2))
plot(fitted_5_model)
```

### leverage

According to the "Residuals vs Leverage" plot, there are no leverage in this data.

### model Assumptions

According to the 'Residuals vs Fitted' plot and 'Scale-Location` plot, we can find that residuals are randomly spread along the change of fitted values and red lines are almost striaight and horizontal, which means the residuals are almost constant across the range of Xs and independent. Linear relationship exits as well. However, read lines are slightly curve around 71, which means the reiduals around 71 might be slightly lower.

For 'Normal Q-Q plot', we can see all dots except first and last few dots spread around the line, considering there are only 50 observations, this is a small sample, as a result, these outliers are normal and overall, residuals are normally distributed.

## Question 5

### 10-fold cross-validation

```{r}
train_data = trainControl(method = "cv", number = 10)

# Fit the 4-variables model that we discussed in previous lectures
model_caret = 
  train(life_exp ~ murder + hs_grad + frost,
                   data = state_clean_df,
                   trControl = train_data,
                   method = 'lm',
                   na.action = na.pass)

model_caret
```

Model coefficients

```{r}
model_caret$finalModel
```

Results of each fold

```{r}
model_caret$resample
```

### residual sampling

Calculate predicted values and reisduals

```{r}
selected_model = lm(life_exp ~ murder + hs_grad + frost, data = state_clean_df)

bootstrap_df = 
  state_clean_df %>% 
  mutate(predicted_y = 71.03 - 0.3001*murder + 0.04658*hs_grad - 0.005943*frost,
         sample_y = life_exp)

residual_fun = function(x, y){
  return(x - y)
}

#predicted_y = predict(final_model)
residual_base = mapply(residual_fun, bootstrap_df$life_exp, bootstrap_df$predicted_y)
#sample_y = residual_l + predicted_y
#test = cbind(state_criterion_df, sample_y)
```

Repeat residuals sampling and count MSE

```{r}
bootsrap_mse_fun = function(rep_num){
  #rep_num = rep_num   # number of repetitions
  #rep_num = 100  # for test
  rmse_v = vector(mode = "numeric", length = rep_num)
  
  len = length(residual_base)
  for (j in 1:rep_num){
    # resample residuals 
    residual_l = sample(residual_base, len, replace = TRUE)
    
    # get new sample ys and residuals
    for (n in 1:len) {
      bootstrap_df$sample_y[n] = residual_l[n] + bootstrap_df$predicted_y[n]
      }

    # fit linear model
    final_model = lm(sample_y ~ murder + hs_grad + frost, data = bootstrap_df)
  
    # get mse
    rmse_v[j] = rmse(final_model)
    }
  rmse_v    # for test 
  return(summary(rmse_v))
}

bootsrap_mse_fun(10) # repeat 10 times

bootsrap_mse_fun(1000) # repeat 10 times
```

### MSE comparing

```{r}
rmse(selected_model)
```

Using "residual sampling bootstrap" benefits when data's residuals do not follow normal distribution. However, based on our test results of model assumption, residuals of this data are normal distributed, thus both two methods can be used to test model. However, 10-fold cross-validation is easier so it is recommended.



